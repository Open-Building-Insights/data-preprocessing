{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9_urban_rural_segregation\n",
    "### Categorizes buildings based on their location into a fitting urbanization status, uses the pre-processed polygons from 8_rural_urban_json_segregation\n",
    "### This notebook needs to be executed twice together with 8_rural_urban_json_segregation to categorize building on a \"overview\" level, i.e., urban-suburban-rural categories and second time to categorize buildings into finer grainde categories using the \"detailed\" option SEGREGATION_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial configuration\n",
    "#### To start working with this particular notebook, you need to provide necessary credential and settings\n",
    "#### Below is an template of configuration, which is necessary prepare aside of this notebook and copy & paste all content in triple quotes to the next cell's input field\n",
    "    \"\"\"\n",
    "    {\n",
    "    \"COS_ENDPOINT_URL\": \"s3.private.eu-de.cloud-object-storage.appdomain.cloud\",\n",
    "    \"COS_AUTH_ENDPOINT_URL\": \"https://iam.cloud.ibm.com/oidc/token\",\n",
    "    \"COS_APIKEY\": \"xxx\",\n",
    "    \"UTILS_BUCKET\": \"notebook-utils-bucket\",\n",
    "    \"COUNTRY_TABLE\": \"FEATURES_DB_VIDA_EXTENDED\",\n",
    "    \"COUNTRY_NAME\": \"Kenya\",\n",
    "    \"SMOD_BUCKET\": \"xxx\",\n",
    "    \"SEGREGATION_STYLE\": \"overview\",\n",
    "    \"VIDA_COUNTRIES_BUILDINGS\": \"vida-countries-buildings\",\n",
    "    }\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read notebook configuration\n",
    "import getpass\n",
    "import json\n",
    "\n",
    "config_str = getpass.getpass('Enter your prepared config: ')\n",
    "config = json.loads(config_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import jaydebeapi as jdbc\n",
    "import jpype\n",
    "import ibm_boto3\n",
    "import gc\n",
    "import io\n",
    "import os\n",
    "import shapely\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "from botocore.client import Config\n",
    "from rasterio.plot import show\n",
    "from tqdm import tqdm\n",
    "from rasterio.mask import mask\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray\n",
    "from skimage import measure as M\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add new country specific bounding box coordinates if needed\n",
    "# table_name = config[\"COUNTRY_TABLE\"]\n",
    "# country = config[\"COUNTRY_NAME\"]\n",
    "# output_SMOD_polygon_geojson =  country + \"_segregated_cleaned.json\"\n",
    "\n",
    "min_latitude = 0\n",
    "max_latitude = 0\n",
    "min_longitude = 0\n",
    "max_longitude = 0\n",
    "# if config[\"COUNTRY_TABLE\"] == 'Kenya':\n",
    "#     min_latitude = -4.7075268\n",
    "#     max_latitude = 5.017422\n",
    "#     min_longitude = 33.9110224\n",
    "#     max_longitude = 41.8914004\n",
    "\n",
    "segregation = {}\n",
    "segregation_priorities = []\n",
    "default_category = \"\"\n",
    "db_col_name = \"\"\n",
    "\n",
    "if config[\"SEGREGATION_STYLE\"] == \"overview\":\n",
    "    segregation = {\n",
    "        'URBAN': [22, 23, 30],\n",
    "        'SUBURBAN': [21],\n",
    "        'RURAL': [12, 13],\n",
    "    }\n",
    "    segregation_priorities = ['URBAN', 'SUBURBAN']\n",
    "    default_category = 'Rural'\n",
    "    db_col_name = 'URBAN_SPLIT'\n",
    "\n",
    "if config[\"SEGREGATION_STYLE\"] == \"detailed\":\n",
    "    segregation = {\n",
    "        'URBAN_CENTER': [30],\n",
    "        'DENSE_URBAN': [23],\n",
    "        'SEMI_DENSE_URBAN': [22],\n",
    "        'SUBURBAN_PERI_URBAN': [21],\n",
    "        'RURAL_CLUSTER': [13],\n",
    "        'LOW_DENSITY_RURAL': [12],\n",
    "    }\n",
    "    segregation_priorities = ['URBAN_CENTER', 'DENSE_URBAN', 'SEMI_DENSE_URBAN', 'SUBURBAN_PERI_URBAN', 'RURAL_CLUSTER', 'LOW_DENSITY_RURAL']\n",
    "    default_category = 'Low Density Rural'\n",
    "    db_col_name = 'GHSL_SMOD'\n",
    "\n",
    "segregation_names = {}\n",
    "segregation_names['URBAN'] = 'Urban'\n",
    "segregation_names['SUBURBAN'] = 'Suburban'\n",
    "segregation_names['RURAL'] = 'Rural'\n",
    "\n",
    "segregation_names['URBAN_CENTER'] = 'Urban Center'\n",
    "segregation_names['DENSE_URBAN'] = 'Dense Urban Cluster'\n",
    "segregation_names['SEMI_DENSE_URBAN'] = 'Semi-dense Urban Cluster'\n",
    "segregation_names['SUBURBAN_PERI_URBAN'] = 'Suburban or Per-urban'\n",
    "segregation_names['RURAL_CLUSTER'] = 'Rural Cluster'\n",
    "segregation_names['LOW_DENSITY_RURAL'] = 'Low Density Rural'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load db2jcc4.jar and polygons json\n",
    "cos_client = ibm_boto3.client(service_name='s3',\n",
    "                              ibm_api_key_id=config[\"COS_APIKEY\"],\n",
    "                              config=Config(signature_version='oauth'),\n",
    "                              endpoint_url=config[\"COS_ENDPOINT_URL\"])\n",
    "\n",
    "response = cos_client.list_objects_v2(Bucket=config[\"UTILS_BUCKET\"])\n",
    "\n",
    "try:\n",
    "    for obj in response['Contents']:\n",
    "        name = obj['Key']\n",
    "        streaming_body_1 = cos_client.get_object(Bucket=config[\"UTILS_BUCKET\"], Key=name)['Body']\n",
    "        print(\"Copying to localStorage :  \" + name)\n",
    "        with io.FileIO(name, 'w') as file:\n",
    "            for i in io.BytesIO(streaming_body_1.read()):\n",
    "                file.write(i)\n",
    "    \n",
    "    from utils import *\n",
    "    print('External utils succesfully imported')\n",
    "except Exception as e:\n",
    "    print('Error occured: ', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "smod_overview_json_filenames = ['Madhya_Pradesh_overview.json', 'South-India_overview.json', 'East-India_overview.json']\n",
    "smod_detailed_json_filenames = ['Madhya_Pradesh_detailed.json', 'South-India_detailed.json', 'East-India_detailed.json']\n",
    "\n",
    "vida_datasets = ['East_India_buildings.parquet', 'Madhya_Pradesh_buildings.parquet', 'South_India_buildings.parquet']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Madhya_Pradesh_overview.json\n",
      "South-India_overview.json\n",
      "East-India_overview.json\n",
      "Madhya_Pradesh_detailed.json\n",
      "South-India_detailed.json\n",
      "East-India_detailed.json\n"
     ]
    }
   ],
   "source": [
    "# Fetch the geoJSON file containing polygons and process each polygon one by one\n",
    "for smod_json in smod_overview_json_filenames + smod_detailed_json_filenames:\n",
    "    \n",
    "    streaming_body = cos_client.get_object(Bucket=config[\"SMOD_BUCKET\"], Key=smod_json)['Body']\n",
    "    print(\"Downloading to local storage :  \" + smod_json)\n",
    "    with io.FileIO(smod_json, 'w') as file:\n",
    "        for i in io.BytesIO(streaming_body.read()):\n",
    "            file.write(i)\n",
    "\n",
    "# Fetch the geoJSON file containing polygons and process each polygon one by one\n",
    "for vida_buildings_parquet in vida_datasets:\n",
    "    \n",
    "    streaming_body = cos_client.get_object(Bucket=config[\"VIDA_COUNTRIES_BUILDINGS\"], Key=vida_buildings_parquet)['Body']\n",
    "    print(\"Downloading to local storage :  \" + vida_buildings_parquet)\n",
    "    with io.FileIO(vida_buildings_parquet, 'w') as file:\n",
    "        for i in io.BytesIO(streaming_body.read()):\n",
    "            file.write(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['East-India_overview.json',\n",
       "  'Madhya_Pradesh_overview.json',\n",
       "  'South-India_overview.json'],\n",
       " ['East-India_detailed.json',\n",
       "  'Madhya_Pradesh_detailed.json',\n",
       "  'South-India_detailed.json'],\n",
       " ['East_India_buildings.parquet',\n",
       "  'Madhya_Pradesh_buildings.parquet',\n",
       "  'South_India_buildings.parquet'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smod_overview_json_filenames.sort(), smod_detailed_json_filenames.sort(), vida_datasets.sort()\n",
    "smod_overview_json_filenames, smod_detailed_json_filenames, vida_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for smod_json_overview, smod_json_detailed, parquet_filename in zip(smod_overview_json_filenames, smod_detailed_json_filenames, vida_datasets):\n",
    "#Open Kenya_segregated polygons\n",
    "    with open(smod_json_overview) as s_f:\n",
    "        geojson_overview = json.load(s_f)\n",
    "\n",
    "    print('Processing: ', smod_json_overview, smod_json_detailed, parquet_filename)\n",
    "\n",
    "    buildings_df = gpd.read_parquet(parquet_filename)\n",
    "    init_len = len(buildings_df)\n",
    "    dfs = []\n",
    "    \n",
    "    for fidx, feature in enumerate(tqdm(geojson_overview['features'], total=len(geojson_overview['features']), desc='Processing overview level')):\n",
    "        \n",
    "        # print(f\"Processing {fidx+1} of {len(geojson['features'])}\")\n",
    "        \n",
    "        polygon_coordinates = feature['geometry']['coordinates'][0]\n",
    "        polygon = shapely.Polygon(polygon_coordinates)\n",
    "        min_lon, min_lat, max_lon, max_lat = polygon.bounds\n",
    "        \n",
    "        buildings_bbox = buildings_df[\n",
    "                (buildings_df.longitude >= min_lon) &\n",
    "                (buildings_df.longitude <= max_lon) &\n",
    "                (buildings_df.latitude >= min_lat) &\n",
    "                (buildings_df.latitude <= max_lat)\n",
    "        ].copy()\n",
    "        \n",
    "        buildings_bbox['buildings_in_polygon'] = [polygon.contains(shapely.Point(row.longitude, row.latitude)) for row in buildings_bbox.itertuples()]\n",
    "\n",
    "        buildings_in_polygon = buildings_bbox[buildings_bbox.buildings_in_polygon == True]        \n",
    "        \n",
    "        buildings_in_polygon = buildings_in_polygon.drop(columns=['buildings_in_polygon'])\n",
    "        buildings_in_polygon['urban_split'] = segregation_names [feature['properties']['seg_type']]\n",
    "        \n",
    "        dfs.append(buildings_in_polygon)\n",
    "     \n",
    "    try: \n",
    "        segregated_df = pd.concat(dfs)\n",
    "        \n",
    "        remaining_buildings = buildings_df[~buildings_df['id'].isin(segregated_df['id'])]\n",
    "        remaining_buildings['urban_split'] = 'Rural'\n",
    "        \n",
    "        main_df = pd.concat([segregated_df, remaining_buildings])\n",
    "        \n",
    "        print(f'init len {init_len} out len {len(main_df)}')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    #################################################################################################################\n",
    "    print('removing duplicates')\n",
    "    main_df = main_df.drop_duplicates(subset='id')\n",
    "    print(f'in len {init_len} out len {len(main_df)}')\n",
    "\n",
    "    main_df.index = [i for i in range(len(main_df))]\n",
    "    \n",
    "    with open(smod_json_detailed) as s_f:\n",
    "        geojson_detailed = json.load(s_f)\n",
    "        \n",
    "    dfs = []\n",
    "        \n",
    "    for fidx, feature in enumerate(tqdm(geojson_detailed['features'], total=len(geojson_detailed['features']), desc='Processing detailed level')):\n",
    "        \n",
    "        # print(f\"Processing {fidx+1} of {len(geojson['features'])}\")\n",
    "        \n",
    "        polygon_coordinates = feature['geometry']['coordinates'][0]\n",
    "        polygon = shapely.Polygon(polygon_coordinates)\n",
    "        min_lon, min_lat, max_lon, max_lat = polygon.bounds\n",
    "        \n",
    "        buildings_bbox = main_df[\n",
    "                (main_df.longitude >= min_lon) &\n",
    "                (main_df.longitude <= max_lon) &\n",
    "                (main_df.latitude >= min_lat) &\n",
    "                (main_df.latitude <= max_lat)\n",
    "        ].copy()\n",
    "        \n",
    "        buildings_bbox['buildings_in_polygon'] = [polygon.contains(shapely.Point(row.longitude, row.latitude)) for row in buildings_bbox.itertuples()]\n",
    "\n",
    "        buildings_in_polygon = buildings_bbox[buildings_bbox.buildings_in_polygon == True]        \n",
    "        \n",
    "        buildings_in_polygon = buildings_in_polygon.drop(columns=['buildings_in_polygon'])\n",
    "        buildings_in_polygon['ghsl_smod'] = segregation_names[feature['properties']['seg_type']]\n",
    "        \n",
    "        dfs.append(buildings_in_polygon)\n",
    "     \n",
    "    try: \n",
    "        segregated_df = pd.concat(dfs)\n",
    "        \n",
    "        remaining_buildings = main_df[~main_df['id'].isin(segregated_df['id'])]\n",
    "        remaining_buildings['ghsl_smod'] = 'Low Density Rural'\n",
    "        \n",
    "        main_df = pd.concat([segregated_df, remaining_buildings])\n",
    "        \n",
    "        main_df = main_df.drop_duplicates(subset='id')\n",
    "        \n",
    "        print(f'in len {init_len} out len {len(main_df)}')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    try:\n",
    "        \n",
    "        filename = parquet_filename.replace('.parquet', '_GHSL_SMOD.parquet')\n",
    "        main_df.to_parquet(filename)\n",
    "        \n",
    "        res=cos_client.upload_file(Filename=filename, Bucket=config[\"VIDA_COUNTRIES_BUILDINGS\"],Key=filename)\n",
    "        print(f'File uploaded to the COS {filename}')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
