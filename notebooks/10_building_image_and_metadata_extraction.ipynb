{"cells":[{"cell_type":"markdown","metadata":{},"source":["## 11_building_image_and_metadata_extraction\n","### Crops building roof images form the Sentinel-2 tiles, uploads them to the COS bucket and sets the corresponding location to the DB2 database"]},{"cell_type":"markdown","metadata":{},"source":["### Initial configuration\n","#### To start working with this particular notebook, you need to provide necessary credential and settings\n","#### Below is an template of configuration, which is necessary prepare aside of this notebook and copy & paste all content in triple quotes to the next cell's input field\n","    \"\"\"\n","    {\n","    \"COS_ENDPOINT_URL\": \"s3.private.eu-de.cloud-object-storage.appdomain.cloud\",\n","    \"COS_AUTH_ENDPOINT_URL\": \"https://iam.cloud.ibm.com/oidc/token\",\n","    \"COS_APIKEY\": \"xxx\",\n","    \"UTILS_BUCKET\": \"notebook-utils-bucket\",\n","    \"BUCKET_TIFF\": \"geotiffs\",\n","    \"COUNTRY_NAME\": \"Kenya\",\n","    \"DB2_CONNECTION_STRING\": \"jdbc:db2://65beb513-5d3d-4101-9001-f42e9dc954b3.brt9d04f0cmqeb8u7740.databases.appdomain.cloud:30371/BLUDB:sslConnection=true;useJDBC4ColumnNameAndLabelSemantics=false;db2.jcc.charsetDecoderEncoder=3;\",\n","    \"DB2_USERNAME\": \"xxx\",\n","    \"DB2_PASSWORD\": \"xxx\",\n","    \"COUNTRY_TABLE\": \"FEATURES_DB_VIDA_EXTENDED\",\n","    \"MGRS_COMPRESSED_IMAGES_BUCKET\": \"building-image-compression\",\n","    \"AREA_THRESHOLD\": 20\n","    }\n","    \"\"\"\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# Read notebook configuration\n","import getpass\n","import json\n","import re\n","\n","config_str = getpass.getpass('Enter your prepared config: ')\n","config = json.loads(config_str)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Import necessary libraries\n","import io\n","from PIL import Image\n","import ibm_boto3\n","from botocore.client import Config\n","import numpy as np\n","import configparser\n","import os\n","from ibm_cloud_sdk_core import ApiException\n","import pandas as pd\n","import geopandas as gpd\n","from tqdm import tqdm\n","from datetime import datetime\n","import rasterio\n","from pyproj import Geod\n","from shapely.geometry import Polygon, MultiPolygon, mapping, Point\n","import matplotlib.pyplot as plt\n","\n","import jaydebeapi as jdbc\n","import jpype\n","\n","from pykml import parser\n","import mgrs\n","import requests\n","import fiona\n","\n","import shapely\n","\n","from fiona.drvsupport import supported_drivers\n","supported_drivers['KML'] = 'rw'"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["External utils succesfully imported\n"]}],"source":["# initiate the S3 client\n","cos_client = ibm_boto3.client(service_name='s3',\n","                              ibm_api_key_id=config[\"COS_APIKEY\"],\n","                              ibm_auth_endpoint=config[\"COS_AUTH_ENDPOINT_URL\"],\n","                              config=Config(signature_version='oauth'),\n","                              endpoint_url=config[\"COS_ENDPOINT_URL\"])\n","\n","response = cos_client.list_objects_v2(Bucket=config[\"UTILS_BUCKET\"])\n","\n","# download utils module\n","try:\n","    from utils import *\n","    print('External utils succesfully imported')\n","    \n","except Exception as e:\n","    print('Desired packages is missing in local env, downloading it...', e)\n","    for obj in response['Contents']:\n","        name = obj['Key']\n","        streaming_body_1 = cos_client.get_object(Bucket=config[\"UTILS_BUCKET\"], Key=name)['Body']\n","        print(\"Downloading to localStorage :  \" + name)\n","        with io.FileIO(name, 'w') as file:\n","            for i in io.BytesIO(streaming_body_1.read()):\n","                file.write(i)\n","    from utils import *\n","    print('External utils succesfully imported')\n","  "]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/wsuser/ipykernel_357/855655699.py:14: DeprecationWarning: jpype._core.isThreadAttachedToJVM is deprecated, use java.lang.Thread.isAttached instead\n","  if jpype.isJVMStarted() and not jpype.isThreadAttachedToJVM():\n"]}],"source":["# connect to the IBM DB2 function\n","def connect_to_db():\n","\n","    jar = 'db2jcc4.jar'\n","    os.environ['CLASSPATH'] = jar\n","\n","    args='-Djava.class.path=%s' % jar\n","    jvm_path = jpype.getDefaultJVMPath()\n","    try:\n","        jpype.startJVM(jvm_path, args)\n","    except Exception as e:\n","        print('startJVM exception: ', e)\n","        \n","    if jpype.isJVMStarted() and not jpype.isThreadAttachedToJVM():\n","        jpype.attachThreadToJVM()\n","        jpype.java.lang.Thread.currentThread().setContextClassLoader(jpype.java.lang.ClassLoader.getSystemClassLoader())\n","        \n","    # create JDBC connection\n","    conn = jdbc.connect(\n","                'com.ibm.db2.jcc.DB2Driver',\n","                config['DB2_CONNECTION_STRING'],\n","                [config[\"DB2_USERNAME\"], config[\"DB2_PASSWORD\"]],\n","                'db2jcc4.jar')\n","    \n","    return conn\n","\n","DB2_connection = connect_to_db()\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["type_source = config[\"TYPE_SOURCE_FILTER\"]\n","BUCKET_TIFF = config[\"BUCKET_TIFF\"]\n","DB_NAME = config[\"DB_NAME\"]\n","country = config[\"COUNTRY_NAME\"]\n","country_table = config[\"COUNTRY_TABLE\"]\n","threshold = float(config[\"AREA_TRESHOLD\"])  # threshold in square meters."]},{"cell_type":"markdown","metadata":{},"source":["#### Generate list of coordinates based on grid"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def download_and_open_kml(url, destination):\n","    response = requests.get(url)\n","    \n","    # Check if the request was successful (status code 200)\n","    if response.status_code == 200:\n","        # Open the file in binary write mode and write the content\n","        with open(destination, 'wb') as file:\n","            file.write(response.content)\n","        print(f\"File downloaded successfully to: {destination}\")\n","\n","        # Open the downloaded KML file using GeoPandas\n","        gdf = gpd.read_file(destination, driver='KML')\n","        return gdf\n","    else:\n","        print(f\"Failed to download file. Status code: {response.status_code}\")\n","        return None\n","\n","def open_kml_from_url(url):\n","    response = requests.get(url)\n","    \n","    # Check if the request was successful (status code 200)\n","    if response.status_code == 200:\n","        # Use BytesIO to create a file-like object from the content\n","        content = BytesIO(response.content)\n","        \n","        # Open the KML file using GeoPandas\n","        gdf = gpd.read_file(content, driver='KML')\n","        return gdf\n","    else:\n","        print(f\"Failed to download file. Status code: {response.status_code}\")\n","        return None\n","\n","def get_lat_lon_bounds(df, tile, MGRS_tiles):\n","    # Assuming `df` contains your GeoDataFrame with MGRS tiles\n","\n","    # Filter the DataFrame for the specific tile\n","    tile_df = df_kml[df_kml['Name'] == tile]\n","\n","    if not tile_df.empty:\n","        # Get the geometry for the tile\n","        geom = tile_df.geometry.iloc[0]\n","\n","        # Calculate the bounds\n","        minx, miny, maxx, maxy = geom.bounds\n","\n","        # Store the bounds for the tile\n","        tile_bounds = {\n","            'min_lat': miny,\n","            'max_lat': maxy,\n","            'min_lon': minx,\n","            'max_lon': maxx\n","        }\n","    else:\n","        # Handle cases where the tile might not exist in the DataFrame\n","        tile_bounds = {\n","            'min_lat': np.nan,\n","            'max_lat': np.nan,\n","            'min_lon': np.nan,\n","            'max_lon': np.nan\n","        }\n","\n","    return tile_bounds"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["File downloaded successfully to: S2A_OPER.kml\n"]}],"source":["# process kml for desired country\n","url = 'https://sentinel.esa.int/documents/247904/1955685/S2A_OPER_GIP_TILPAR_MPC__20151209T095117_V20150622T000000_21000101T000000_B00.kml'\n","destination = 'S2A_OPER.kml'\n","\n","df_kml = download_and_open_kml(url, destination)\n","\n","m = mgrs.MGRS()\n","\n","countries_geoJSON_url = 'https://datahub.io/core/geo-countries/r/countries.geojson'\n","countries_geoJSON = requests.get(countries_geoJSON_url).content\n","countries_geoJSON = json.loads(countries_geoJSON)\n","\n","country_geometry = {}\n","for feature in countries_geoJSON['features']:\n","    if feature['properties']['ADMIN'] == country:\n","        country_geometry['geometry'] = feature['geometry']\n","        break\n","\n","if country_geometry['geometry']['type'] == 'MultiPolygon':\n","    coordinates = shapely.MultiPolygon(country_geometry['geometry']['coordinates'])\n","elif country_geometry['geometry']['type'] == 'Polygon':\n","    coordinates = shapely.Polygon(country_geometry['geometry']['coordinates'])\n","    \n","# collect MGRS tiles for desired country\n","MGRS_tiles = []\n","\n","for polygon in coordinates.geoms:        \n","    coordinates_list = polygon.exterior.coords._coords\n","    for xy in coordinates_list:\n","        MGRS_tiles.append(m.toMGRS(xy[1], xy[0], MGRSPrecision=0))\n","\n","    latmin, lonmin, latmax, lonmax = polygon.bounds\n","        \n","    resolution = 0.05\n","    for lat in np.arange(latmin, latmax, resolution):\n","        for lon in np.arange(lonmin, lonmax, resolution):\n","                        \n","            MGRS_tiles.append(m.toMGRS(round(lon,4), round(lat,4), MGRSPrecision=0))\n","        \n","MGRS_tiles = list(set(MGRS_tiles))"]},{"cell_type":"markdown","metadata":{},"source":["#### Load New Tif Files"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tif_images_bucket = cos_client.list_objects_v2(Bucket=BUCKET_TIFF)\n","tif_images_objects = tif_images_bucket['Contents']\n","\n","tif_images_filenames = [obj['Key'] for obj in tif_images_objects]\n","tif_images_filenames"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["compressed_images_objects = cos_client.list_objects_v2(Bucket=config[\"MGRS_COMPRESSED_IMAGES_BUCKET\"])['Contents']\n","compressed_images_filenames = [obj['Key'].replace('compressed_images_', '').replace('.zip', '_cloudless.tif') for obj in compressed_images_objects]"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["for excl in compressed_images_filenames:\n","    tif_images_filenames.remove(excl)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# Prepare GeoDataframe of buildings from bounding box\n","def fetch_builings_in_bbox(lon_min, lon_max, lat_min, lat_max):\n","    \n","    sql = f\"\"\"\n","        SELECT DISTINCT ID, POLYGON_COORDINATES FROM USER1.{country_table} \n","        WHERE \n","            (LATITUDE >= {lat_min}) AND \n","            (LATITUDE <= {lat_max}) AND \n","            (LONGITUDE >= {lon_min}) AND \n","            (LONGITUDE <= {lon_max}) AND\n","            AREA_IN_METERS > {threshold}\n","            \"\"\"\n","    cursor = DB2_connection.cursor()\n","    cursor.execute(sql)\n","    data = cursor.fetchall()\n","\n","    gpd.options.display_precision = 7\n","\n","    df = pd.DataFrame(data=data, columns=['doc_id', 'polygon_coordinates'])\n","\n","    convert_dict = {\n","                    'doc_id': str,\n","                    'polygon_coordinates': str\n","                    }\n","\n","    df = df.astype(convert_dict)\n","    df['geometry'] = gpd.GeoSeries.from_wkt(df['polygon_coordinates'])\n","    df = df.drop(columns=['polygon_coordinates'])\n","\n","    df = gpd.GeoDataFrame(\n","        df, geometry=df.geometry, crs=\"EPSG:4326\"\n","    )\n","\n","    df = df.where(~df['geometry'].isna()).dropna()\n","\n","    df[\"corresponding_tiff\"] = ['NA' for _ in range(len(df))]\n","    df \n","    \n","    return df"]},{"cell_type":"markdown","metadata":{},"source":["#### Process, compress and upload to S3 bucket"]},{"cell_type":"markdown","metadata":{},"source":["Define functions for compression, upload to S3 and update building information in DB2:"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["import base64\n","import io\n","import shutil\n","import time\n","import pandas as pd\n","import numpy as np\n","\n","def match_corresponding_tiff(df, areas_covered_by_tifs, path_to_tif_folder):\n","    for row in df.itertuples():\n","        \n","        try:\n","            df.at[row.Index, 'corresponding_tiff'] = get_path_to_tif(row.geometry, areas_covered_by_tifs, path_to_tif_folder)\n","        except Exception as e:\n","            print(f'Exception occurred {e} for row: {row}')\n","    return df\n","\n","# This is an auxiliary function, not used directly - it can be used to obtain the filename of the archive for a given tiff file\n","def get_compressed_file_name(tif_name):\n","    # Define a regular expression pattern to match the desired part\n","    prefix = filename.split('_')[0]\n","\n","    # Use re.search to find the first match of the pattern in the input string\n","    match = re.search(pattern, tif_name)\n","\n","    # Extract the matched part\n","    if match:\n","        extracted_part = match.group(1)\n","        \n","    return extracted_part\n","\n","def compress_and_upload_images(df, object_name, bucket_name):\n","    \n","    metadata_df = pd.DataFrame()\n","    t2 = time.time()\n","    \n","    try:\n","        buildings_no_tag_toDB = df.loc[df['image_name'].notnull()]\n","        images = [(row.image_name, base64.b64decode(row.image_source_bytes)) for row in buildings_no_tag_toDB.itertuples()]\n","\n","        print(f\"Number of images were found: {len(images)}\")\n","        if len(images) != 0:\n","            print('Start images upload')\n","\n","            try:\n","                # Create a DataFrame with metadata columns and append the image_name and image_data\n","                metadata_df = buildings_no_tag_toDB[['doc_id',\n","                                                     'geometry',\n","                                                     'corresponding_tiff',\n","                                                     'image_name',\n","                                                     'tiff_name',\n","                                                     'image_source_bytes',\n","                                                    ]]\n","                        \n","                # Replace NaN values with a placeholder before compression\n","                metadata_df.replace({np.nan: 'NA'}, inplace=True)\n","                \n","                # Compress the DataFrame to a CSV file\n","                zip_buffer = io.BytesIO()\n","                metadata_df.to_csv(zip_buffer, index=False)\n","                zip_buffer.seek(0)\n","                compressed_data = zip_buffer.read()\n","\n","                print(f'Compression time: {time.strftime(\"%H:%M:%S\", time.gmtime(int(time.time() - t2)))}')\n","                \n","                # Upload compressed data to S3\n","                cos_client.upload_fileobj(io.BytesIO(compressed_data), bucket_name, object_name)\n","\n","                print(f'Successfully uploaded {object_name} to {bucket_name}')\n","\n","            except Exception as e:\n","                print(f'Error processing images: {e}')\n","\n","        else:\n","            print('Zero images were found')\n","\n","    except Exception as e:\n","        print('Image upload error:', e)\n","\n","    finally:\n","        # Restore NaN values after compression\n","        metadata_df.replace({'NA': np.nan}, inplace=True)\n","\n","    print('Delete entire tiff/ directory')\n","    shutil.rmtree('tiff/', ignore_errors=True)\n","\n","    print(f'Database upload serially: {time.strftime(\"%H:%M:%S\", time.gmtime(int(time.time() - t2)))}')\n","\n","# This is an auxiliary function, not used directly - it can be used to obtain building roof images\n","def download_and_decompress_images(bucket_name, object_name):\n","    try:\n","        t1 = time.time()\n","        # Step 1: Download compressed data from S3\n","        compressed_data = cos_client.get_object(Bucket=bucket_name, Key=object_name)['Body'].read()\n","        print(f'Download time: {time.strftime(\"%H:%M:%S\", time.gmtime(int(time.time() - t1)))}')\n","        \n","        t2 = time.time()\n","        # Decompress data\n","        csv_data = compressed_data.decode()  # Convert bytes to string\n","        df = pd.read_csv(io.StringIO(csv_data))\n","        \n","        # After decompression, replace the placeholder with NaN values\n","        df.replace({'NA': np.nan}, inplace=True)\n","\n","        print(f'Decompression time: {time.strftime(\"%H:%M:%S\", time.gmtime(int(time.time() - t2)))}')\n","\n","        return df\n","\n","    except Exception as e:\n","        print(f'Error downloading and decompressing images: {e}')\n","\n","# Obsolete function from the CLoudant era        \n","'''\n","def bulk_update_features_db_vida_extended(df, db_name_upload, object_name):\n","\n","    bulk_updates = []\n","    path_compressed_file = \"cos://eu-de/building-image-compression/\"\n","    \n","    try:\n","        buildings_no_tag_toDB = df.loc[df['image_name'].notnull()]\n","        images = [r for r in buildings_no_tag_toDB.itertuples()]\n","\n","        print(f\"Number if images were found: {len(images)}\")\n","        if len(images) != 0:\n","            print('Start images upload')\n","\n","            for idx, osm_row in enumerate(tqdm(images, desc=\"Images uploaded\")):\n","                try:\n","                    document = client.get_document(\n","                        db=db_name_upload,\n","                        doc_id=osm_row.doc_id\n","                    ).get_result()\n","\n","                    document['tiff_file'] = str(osm_row.tiff_name)\n","                    document['image_url'] = path_compressed_file + object_name\n","                    \n","                    # Post the batch of documents to Cloudant\n","                    bulk_updates.append(document)\n","                    bulk_docs = BulkDocs(docs=bulk_updates)\n","                    # Batch update every BATCH_SIZE_DB_UPDATE \n","                    \n","                    if (len(bulk_updates) >= BATCH_SIZE_DB_UPDATE) or (idx == (len(buildings_no_tag_toDB) - 1)):\n","                        update_document_response = client.post_bulk_docs(\n","                            db=db_name_upload,\n","                            bulk_docs=bulk_docs\n","                        ).get_result()\n","                        bulk_updates = []\n","\n","                    time.sleep(0.003)\n","\n","                except ApiException as ae:\n","                    if ae.code == 404:\n","                        # Document not found, continue with the next file\n","                        print(f\"Document with ID {osm_row.doc_id} not found. Skipping...\")\n","                        continue\n","\n","                    print(f\"Operation failed for {osm_row.doc_id}\")\n","                    print(\" - status code: \" + str(ae.code))\n","                    print(\" - error message: \" + ae.message)\n","                    if \"reason\" in ae.http_response.json():\n","                        print(\" - reason: \" + ae.http_response.json()[\"reason\"])\n","\n","        else:\n","            print('Zero images were found')\n","\n","    except Exception as e:\n","        print('Image upload error:', e)\n","'''\n","\n","def bulk_update_db2(tiff_name, object_name, bbox):\n","\n","    bulk_updates = []\n","    path_compressed_file = f\"cos://eu-de/{config[\"MGRS_COMPRESSED_IMAGES_BUCKET\"]}/{object_name}\"\n","\n","    try:\n","        \n","        sql = f\"\"\"\n","            UPDATE \"USER1\".\"{country_table}\"\n","              SET\n","                  \"TIFF_FILE\" = '{tiff_name}',     \n","                  \"IMAGE_URL\" = '{path_compressed_file}'                 \n","            WHERE \n","                (LATITUDE >= {bbox['lat_min']}) AND \n","                (LATITUDE <= {bbox['lat_max']}) AND \n","                (LONGITUDE >= {bbox['lon_min']}) AND \n","                (LONGITUDE <= {bbox['lon_max']}) AND\n","                (AREA_IN_METERS > {threshold})\n","            \n","        \"\"\"\n","        print(sql)\n","        cursor = DB2_connection.cursor()\n","        cursor.execute(sql)\n","\n","    except Exception as e:\n","        print('Image upload error:', e)"]},{"cell_type":"markdown","metadata":{},"source":["Main code to loop through tiff-files, process and upload images:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["margin = 0  # defines how many pixels we add to the building wenn preparing the dataset.\n","# where the preprocessed samples shall be stored shall be stored\n","folder_preprocessed_files = 'samples/'\n","os.makedirs(os.path.dirname(folder_preprocessed_files), exist_ok=True)\n","\n","path_to_tif_folder = 'tiff/'\n","\n","try:\n","    shutil.rmtree('tiff/', ignore_errors=True)\n","except Exception as e:\n","    print(e)\n","\n","\n","dfs = []\n","for tiff_name in tif_images_filenames: # iterate through grid system\n","    init_time = time.time()\n","    print('Create /tiff directory')\n","    os.makedirs(os.path.dirname(path_to_tif_folder), exist_ok=True)\n","    \n","    t1 = time.time()\n","    \n","    streaming_body = cos_client.get_object(Bucket=BUCKET_TIFF, Key=tiff_name)['Body']\n","    \n","\n","    with io.FileIO(path_to_tif_folder + tiff_name, 'w') as file:\n","        print(\"Copying to localStorage: \" + path_to_tif_folder + tiff_name)\n","        for i in io.BytesIO(streaming_body.read()):\n","            file.write(i)\n","                \n","    print(f'Files downloaded, time took: {time.strftime(\"%H:%M:%S\", time.gmtime(int(time.time() - t1)))}')\n","    t1 = time.time()\n","    \n","    areas_covered_by_tifs = create_bounds_dict(path_to_tifs=path_to_tif_folder)\n","    areas_covered_by_tif = areas_covered_by_tifs[tiff_name]\n","\n","    lon_min = areas_covered_by_tif['lons_sorted'][0]\n","    lon_max = areas_covered_by_tif['lons_sorted'][1]\n","\n","    lat_min = areas_covered_by_tif['lats_sorted'][0]\n","    lat_max = areas_covered_by_tif['lats_sorted'][1]\n","    bbox = {\n","        'lon_min': lon_min,\n","        'lon_max': lon_max,\n","        'lat_min': lat_min,\n","        'lat_max': lat_max\n","    }\n","    print(lon_min, lon_max, lat_min, lat_max)\n","    t1 = time.time()\n","    df = fetch_builings_in_bbox(lon_min, lon_max, lat_min, lat_max)\n","\n","    df = match_corresponding_tiff(df, areas_covered_by_tifs, path_to_tif_folder)\n","    tifs = df.corresponding_tiff.unique().tolist()\n","    if len(tifs) == 0:\n","        print(\"No tiff file was found, that corresponds with Lon and Lat coordinates in GeoDataFrame\")\n","    else:\n","            \n","        # sanity check: is it a valid tif path\n","        tif = path_to_tif_folder + tiff_name\n","        if isinstance(tif, str):\n","            if tif.endswith('.tif'):\n","                with rasterio.open(tif) as dataset:\n","                    bands = dataset.read()\n","\n","                    # Assuming the TIFF files have 3 bands (RGB)\n","                    if bands.shape[0] == 3:  # Checking if it has 3 bands (R, G, B)\n","                        # Reorder array from 3, height, width to height, width, 3\n","                        picture_all_bands = np.transpose(bands, (1, 2, 0))\n","\n","                        # Convert to RGB\n","                        picture = np.clip(picture_all_bands, 0.0, 255.0).astype('uint8')\n","\n","                        # get all rows with buildings located within this tif file\n","                        for index, row, in tqdm(df.loc[(df['corresponding_tiff'] == tif)].iterrows(), desc='Cropping images', total=len(df)):\n","                            pixel_coordinates = get_pixel_coordinates(row.geometry, areas_covered_by_tifs, dataset)\n","\n","                            if len(pixel_coordinates) == 0:\n","                                continue\n","                        # get min max cols\n","                            rowcolminmax = get_min_max_values_of_row_col(pixel_coordinates=pixel_coordinates)\n","\n","                            label_rgb = picture[rowcolminmax['rowminmax'][0] - margin:rowcolminmax['rowminmax'][1] + margin + 1,\n","                                    rowcolminmax['colminmax'][0] - margin:rowcolminmax['colminmax'][1] + margin + 1, :]\n","\n","                            try:\n","                                im = Image.fromarray(label_rgb.astype(\"uint8\"))\n","                                rawBytes = io.BytesIO()\n","                                im.save(rawBytes, \"png\")\n","                                rawBytes.seek(0)\n","                                df.at[index, 'image_name'] = f\"{index}.png\"\n","                                df.at[index, 'tiff_name'] = tif.split('/')[1]\n","                                df.at[index, 'image_source_bytes'] = base64.b64encode(rawBytes.read()).decode('ascii')\n","\n","                            except Exception as e:\n","                                print('Image processing error:', e)\n","                \n","        object_name_tif = tiff_name.split('_')[0]\n","        object_name = f'compressed_images_{object_name_tif}.zip'\n","        bucket_name = config[\"MGRS_COMPRESSED_IMAGES_BUCKET\"]\n","        DB_NAME_UPLOAD = country_table\n","\n","        BATCH_SIZE_DB_UPDATE = 2000\n","\n","        compress_and_upload_images(df, object_name, bucket_name)\n","        t3 = time.time()\n","        df = df.loc[df['image_name'].notnull()]\n","\n","        bulk_update_db2(tiff_name, object_name, bbox)\n","        \n","        print('total time:', time.time() - t3)\n","        \n","    print('Remove tiff/ directory')\n","    shutil.rmtree('tiff/', ignore_errors=True)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":1}
