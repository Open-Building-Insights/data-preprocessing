{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12_inference_main\n",
    "### Building classification based on the pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial configuration\n",
    "#### To start working with this particular notebook, you need to provide necessary credential and settings\n",
    "#### Below is an template of configuration, which is necessary prepare aside of this notebook and copy & paste all content in triple quotes to the next cell's input field\n",
    "    \"\"\"\n",
    "    {\n",
    "    \"COS_ENDPOINT_URL\": \"s3.private.eu-de.cloud-object-storage.appdomain.cloud\",\n",
    "    \"COS_AUTH_ENDPOINT_URL\": \"https://iam.cloud.ibm.com/oidc/token\",\n",
    "    \"COS_APIKEY\": \"xxx\",\n",
    "    \"DB2_CONNECTION_STRING\": \"jdbc:db2://65beb513-5d3d-4101-9001-f42e9dc954b3.brt9d04f0cmqeb8u7740.databases.appdomain.cloud:30371/BLUDB:sslConnection=true;useJDBC4ColumnNameAndLabelSemantics=false;db2.jcc.charsetDecoderEncoder=3;\",\n",
    "    \"DB2_USERNAME\": \"xxx\",\n",
    "    \"DB2_PASSWORD\": \"xxx\",\n",
    "    \"UTILS_BUCKET\": \"notebook-utils-bucket\",\n",
    "    \"COUNTRY_TABLE\": \"FEATURES_DB_MAHARASHTRA\",\n",
    "    \"ML_MODELS_BUCKET\": \"ml-saved-models\",\n",
    "    \"ML_MODELS_BUCKET_CRN\": \"xxx\",\n",
    "    \"L2_ML_MODELS_BUCKET\": \"l2-ml-saved-models\",\n",
    "    \"JOB_STATUS_BUCKET\": \"inferencing-state\",\n",
    "    \"MGRS_COMPRESSED_IMAGES_BUCKET\": \"building-image-compression\",\n",
    "    \"INFERECE_JSON_NAME\": \"inferenced_state_Maharashtra_p1.json\",\n",
    "    \"MODEL_NAME\": \"HybridArchitecture_CFG001_India_Maharashtra_DenseNet121_L1_1I_2N_dt06_27_2024_01_15_17.h5\"\n",
    "    }\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read notebook configuration\n",
    "import getpass\n",
    "import json\n",
    "\n",
    "config_str = getpass.getpass('Enter your prepared config: ')\n",
    "config = json.loads(config_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "#! pip install ibmcloudant\n",
    "#! pip install geopandas==0.13.2\n",
    "#! pip install rasterio==1.3.8\n",
    "# ! pip intasll shapely\n",
    "# ! pip install numpy==1.23.5;\n",
    "# ! pip install pyproj==3.6.0\n",
    "#! pip install pathos==0.3.1\n",
    "#! pip install ibmcloudant==0.4.3\n",
    "#! pip install ibm-cloud-sdk-core==3.16.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External utils succesfully imported\n",
      "Model downloaded:  HybridArchitecture_CFG001_India_Maharashtra_DenseNet121_L1_1I_2N_dt06_27_2024_01_15_17.h5\n",
      "INFERENCE SCRIPT START\n",
      "MODEL loaded\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import base64\n",
    "import os\n",
    "import configparser\n",
    "import sys\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "import cv2\n",
    "import ibm_boto3\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from botocore.client import Config\n",
    "import io\n",
    "from ibm_cloud_sdk_core import ApiException\n",
    "from datetime import datetime\n",
    "from ibmcloudant.cloudant_v1 import CloudantV1, BulkDocs\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as SKM\n",
    "\n",
    "import jaydebeapi as jdbc\n",
    "import jpype\n",
    "import threading\n",
    "import requests\n",
    "from collections import Counter\n",
    "# import shapely\n",
    "# from pyproj import Geod\n",
    "# geod = Geod(ellps=\"WGS84\")\n",
    "\n",
    "\n",
    "cpu_count = os.cpu_count()\n",
    "# assign configuration variables\n",
    "# sys.path.append('.')\n",
    "\n",
    "BUCKET_MODEL = config[\"ML_MODELS_BUCKET\"]\n",
    "MODEL_NAME = config[\"MODEL_NAME\"]\n",
    "\n",
    "BATCH_INFERENCE = 10_000\n",
    "BATCH_SIZE_ML_CLASSIFIER = 128\n",
    "BATCH_SIZE_DB_UPDATE = 500\n",
    "LIMIT = 1_000\n",
    "img_size = (124, 124)\n",
    "\n",
    "cos_client = ibm_boto3.client(service_name='s3',\n",
    "                              ibm_api_key_id=config[\"COS_APIKEY\"],\n",
    "                              ibm_auth_endpoint=config[\"COS_AUTH_ENDPOINT_URL\"],\n",
    "                              config=Config(signature_version='oauth'),\n",
    "                              endpoint_url=config[\"COS_ENDPOINT_URL\"])\n",
    "\n",
    "cos_client_resource = ibm_boto3.resource(service_name='s3',\n",
    "    ibm_api_key_id=config[\"COS_APIKEY\"],\n",
    "    ibm_service_instance_id=config[\"ML_MODELS_BUCKET_CRN\"],\n",
    "    ibm_auth_endpoint=config[\"COS_AUTH_ENDPOINT_URL\"],\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url=config[\"COS_ENDPOINT_URL\"]\n",
    ")\n",
    "\n",
    "response = cos_client.list_objects_v2(Bucket=config[\"UTILS_BUCKET\"])\n",
    "# download utils module\n",
    "try:\n",
    "    from utils import *\n",
    "    print('External utils succesfully imported')\n",
    "    \n",
    "except Exception as e:\n",
    "    print('Desired packages is missing in local env, downloading it...', e)\n",
    "    for obj in response['Contents']:\n",
    "        name = obj['Key']\n",
    "        streaming_body_1 = cos_client.get_object(Bucket=config[\"UTILS_BUCKET\"], Key=name)['Body']\n",
    "        print(\"Downloading to localStorage :  \" + name)\n",
    "        with io.FileIO(name, 'w') as file:\n",
    "            for i in io.BytesIO(streaming_body_1.read()):\n",
    "                file.write(i)\n",
    "    from utils import *\n",
    "    print('External utils succesfully imported')\n",
    "\n",
    "\n",
    "l2ml_models_bucket = cos_client_resource.Bucket(config[\"L2_ML_MODELS_BUCKET\"])\n",
    "\n",
    "l2ml_models_bucket.download_file(MODEL_NAME, MODEL_NAME)\n",
    "print('Model downloaded: ', MODEL_NAME)\n",
    "\n",
    "# time measuring\n",
    "def format_timedelta(td):\n",
    "    minutes, seconds = divmod(td.seconds + td.days * 86400, 60)\n",
    "    hours, minutes = divmod(minutes, 60)\n",
    "    return '{:d} hours {:02d} minutes {:02d} seconds'.format(hours, minutes, seconds)\n",
    "\n",
    "\n",
    "startTime1 = datetime.now()\n",
    "startTime = datetime.now()\n",
    "\n",
    "print(\"INFERENCE SCRIPT START\")\n",
    "\n",
    "if os.path.isfile(MODEL_NAME) == False:\n",
    "    print(\"ERROR: Can not find pre-trained model, Aborting ....\")\n",
    "    quit()\n",
    "cnn_mod = load_model(MODEL_NAME)\n",
    "\n",
    "print('MODEL loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/wsuser/ipykernel_427/1572909626.py:14: DeprecationWarning: jpype._core.isThreadAttachedToJVM is deprecated, use java.lang.Thread.isAttached instead\n",
      "  if jpype.isJVMStarted() and not jpype.isThreadAttachedToJVM():\n"
     ]
    }
   ],
   "source": [
    "def connect_to_db():\n",
    "\n",
    "    jar = 'db2jcc4.jar'\n",
    "#     os.environ['JAVA_HOME'] = '/usr/libexec/java_home'\n",
    "    os.environ['CLASSPATH'] = jar\n",
    "\n",
    "    args='-Djava.class.path=%s' % jar\n",
    "    jvm_path = jpype.getDefaultJVMPath()\n",
    "    try:\n",
    "        jpype.startJVM(jvm_path, args)\n",
    "    except Exception as e:\n",
    "        print('startJVM exception: ', e)\n",
    "        \n",
    "    if jpype.isJVMStarted() and not jpype.isThreadAttachedToJVM():\n",
    "        jpype.attachThreadToJVM()\n",
    "        jpype.java.lang.Thread.currentThread().setContextClassLoader(jpype.java.lang.ClassLoader.getSystemClassLoader())\n",
    "        \n",
    "    \n",
    "    conn = jdbc.connect(\n",
    "                'com.ibm.db2.jcc.DB2Driver',\n",
    "                config['DB2_CONNECTION_STRING'],\n",
    "                [config[\"DB2_USERNAME\"], config[\"DB2_PASSWORD\"]],\n",
    "                'db2jcc4.jar')\n",
    "    \n",
    "\n",
    "    return conn\n",
    "\n",
    "DB2_connection = connect_to_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = cos_client.list_objects_v2(Bucket=config['MGRS_COMPRESSED_IMAGES_BUCKET'])\n",
    "objects = response.get(\"Contents\", [])\n",
    "# MGRS_tiles_to_process = [obj['Key'] for obj in objects]\n",
    "\n",
    "MGRS_tiles = {obj['Key']: obj['Size'] / 1_000_000 for obj in objects}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('compressed_images_43QHA.zip', 0.0036),\n",
       " ('compressed_images_43QFE.zip', 0.06688),\n",
       " ('compressed_images_44QMF.zip', 3.079516),\n",
       " ('compressed_images_43QBV.zip', 4.511823),\n",
       " ('compressed_images_44QMG.zip', 18.904504),\n",
       " ('compressed_images_44QKG.zip', 36.747604),\n",
       " ('compressed_images_43QEU.zip', 37.258969),\n",
       " ('compressed_images_43QHB.zip', 42.385128),\n",
       " ('compressed_images_43QCD.zip', 54.649278),\n",
       " ('compressed_images_44QKJ.zip', 88.867104),\n",
       " ('compressed_images_43QFD.zip', 154.722814),\n",
       " ('compressed_images_43QHC.zip', 205.5549),\n",
       " ('compressed_images_44QKH.zip', 216.034441),\n",
       " ('compressed_images_43QFC.zip', 241.214881),\n",
       " ('compressed_images_43QEB.zip', 262.349007),\n",
       " ('compressed_images_43QEA.zip', 275.13706),\n",
       " ('compressed_images_43QCB.zip', 310.062676),\n",
       " ('compressed_images_43QEV.zip', 324.124237),\n",
       " ('compressed_images_43QCC.zip', 337.31677)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MGRS_tiles = sorted(MGRS_tiles.items(), key=lambda x:x[1])\n",
    "MGRS_tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('compressed_images_43QHA.zip', 0.0036),\n",
       " ('compressed_images_43QFE.zip', 0.06688),\n",
       " ('compressed_images_44QMF.zip', 3.079516),\n",
       " ('compressed_images_43QBV.zip', 4.511823),\n",
       " ('compressed_images_44QMG.zip', 18.904504),\n",
       " ('compressed_images_44QKG.zip', 36.747604),\n",
       " ('compressed_images_43QEU.zip', 37.258969),\n",
       " ('compressed_images_43QHB.zip', 42.385128),\n",
       " ('compressed_images_43QCD.zip', 54.649278),\n",
       " ('compressed_images_44QKJ.zip', 88.867104),\n",
       " ('compressed_images_43QFD.zip', 154.722814),\n",
       " ('compressed_images_43QHC.zip', 205.5549),\n",
       " ('compressed_images_44QKH.zip', 216.034441),\n",
       " ('compressed_images_43QFC.zip', 241.214881),\n",
       " ('compressed_images_43QEB.zip', 262.349007),\n",
       " ('compressed_images_43QEA.zip', 275.13706),\n",
       " ('compressed_images_43QCB.zip', 310.062676),\n",
       " ('compressed_images_43QEV.zip', 324.124237),\n",
       " ('compressed_images_43QCC.zip', 337.31677)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MGRS_tiles_to_process_p1 = []\n",
    "MGRS_tiles_to_process_p2 = []\n",
    "part = 0\n",
    "for idx, i in enumerate(MGRS_tiles):\n",
    "    if (idx + 1) % 2 == 0:\n",
    "        MGRS_tiles_to_process_p1.append(i)\n",
    "    else:\n",
    "        MGRS_tiles_to_process_p2.append(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_decompress_images(bucket_name, object_name):\n",
    "    try:\n",
    "        t1 = time.time()\n",
    "        # Step 1: Download compressed data from S3\n",
    "        compressed_data = cos_client.get_object(Bucket=bucket_name, Key=object_name)['Body'].read()\n",
    "#         print(f'Download time: {time.strftime(\"%H:%M:%S\", time.gmtime(int(time.time() - t1)))}')\n",
    "        \n",
    "        t2 = time.time()\n",
    "        # Decompress data\n",
    "        csv_data = compressed_data.decode()  # Convert bytes to string\n",
    "        df = pd.read_csv(io.StringIO(csv_data))\n",
    "        \n",
    "        # After decompression, replace the placeholder with NaN values\n",
    "        df.replace({'NA': np.nan}, inplace=True)\n",
    "\n",
    "#         print(f'Decompression time: {time.strftime(\"%H:%M:%S\", time.gmtime(int(time.time() - t2)))}')\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error downloading and decompressing images: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_DB2_item(row: dict, cursor):\n",
    "    '''\n",
    "    row: dict = {\n",
    "                \"LATITUDE\": \"\",\n",
    "                \"LONGITUDE\": \"\",\n",
    "                \"ML_MODEL\": \"\",\n",
    "                \"ML_CONFIDENCE\": \"\",\n",
    "                \"CLASSIFICATION_TYPE\": \"\"\n",
    "                }\n",
    "    '''\n",
    "    \n",
    "    sql = f\"\"\"\n",
    "        UPDATE \"USER1\".\"{config[\"COUNTRY_TABLE\"]}\"\n",
    "            SET\n",
    "              \"ML_MODEL\" = '{row['ML_MODEL']}',     \n",
    "              \"ML_CONFIDENCE\" = '{row['ML_CONFIDENCE']}',\n",
    "              \"CLASSIFICATION_SOURCE\" = 'classification_model',\n",
    "              \"CLASSIFICATION_TYPE\" = '{row['CLASSIFICATION_TYPE']}'\n",
    "            WHERE \n",
    "                (LATITUDE = {row['LATITUDE']}) AND \n",
    "                (LONGITUDE = {row['LONGITUDE']})\n",
    "        \"\"\"\n",
    "    \n",
    "    cursor.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferenced_state = {\n",
    "    \"inferenced_count\": 0,\n",
    "    \"current_file\": \"\",\n",
    "    \"total_inferenced_count\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_smod(smod_name):\n",
    "\n",
    "    SMOD_mapper = {\n",
    "        'Very Low Density Rural Grids (Mostly Uninhabited Area)': 1,\n",
    "        'Low Density Rural Grids Cells (Dispersed Rural Area)': 2,\n",
    "        'Rural Cluster (Village)': 3,\n",
    "        'Suburban Or Peri-Urban Cells (Suburb)': 4,\n",
    "        'Dense And Semi-Dense Urban Cluster (Town)': 5,\n",
    "        'Urban Centre (City)': 6,\n",
    "    }\n",
    "    \n",
    "    return SMOD_mapper[smod_name]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_filename = config[\"INFERECE_JSON_NAME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_state_to_bucket(inferenced_count: dict):\n",
    "    \n",
    "    with open(json_filename, \"w\") as outfile:\n",
    "                json.dump(inferenced_count, outfile)\n",
    "                \n",
    "    cos_client.upload_file(\n",
    "        Filename=json_filename,\n",
    "        Bucket=config[\"JOB_STATUS_BUCKET\"],\n",
    "        Key=json_filename,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = DB2_connection.cursor()\n",
    "batch_size = 2000\n",
    "\n",
    "total_inferenced_count = 0\n",
    "normalize_area = 20_000\n",
    "normalize_height = 20\n",
    "normalize_smod = 6\n",
    "\n",
    "images, numeric = [], []\n",
    "\n",
    "for fname in MGRS_tiles_to_process_p1:\n",
    "\n",
    "\n",
    "    print('Processing file', fname)\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    df = download_and_decompress_images(config['MGRS_COMPRESSED_IMAGES_BUCKET'], fname[0])\n",
    "    \n",
    "    print(f'Assign SMODid for {fname}')\n",
    "    \n",
    "    df['SMOD_id'] = df['ghsl_smod'].apply(map_smod)\n",
    "    \n",
    "#     df['near_junction'] = [0 for _ in range(len(df))]\n",
    "    \n",
    "    df['longitude'] = df['doc_id'].apply(lambda x: float(x.split(':')[0]))\n",
    "    df['latitude'] = df['doc_id'].apply(lambda x: float(x.split(':')[1]))\n",
    "    \n",
    "    df = df.tail(400_000)\n",
    "    \n",
    "    df.index = [i for i in range(len(df))]\n",
    "    \n",
    "    images_batch = []\n",
    "    predicted_baches = []\n",
    "\n",
    "    batches_processed = 0\n",
    "    \n",
    "    for idx, row in enumerate(tqdm(df.itertuples(), desc='Inferencing & updating', total=len(df))):\n",
    "\n",
    "        image_source_bytes = base64.b64decode(row.image_source_bytes)\n",
    "\n",
    "        image = Image.open(io.BytesIO(image_source_bytes))  \n",
    "        image = image.resize((124, 124), Image.Resampling.NEAREST)\n",
    "        image = np.array(image)\n",
    "        \n",
    "        numeric_input = [row.area_in_meters / normalize_area, row.SMOD_id/normalize_smod]\n",
    "        \n",
    "        images.append(image)\n",
    "        numeric.append(numeric_input)\n",
    "\n",
    "        if (len(images) == batch_size) or (idx == len(df) - 1):\n",
    "\n",
    "            inputs_batch = [\n",
    "                np.array(images), \n",
    "                np.array(numeric)\n",
    "                ]\n",
    "\n",
    "            predictions = cnn_mod.predict(inputs_batch, \n",
    "                                          batch_size=len(images),\n",
    "                                          verbose=0, \n",
    "                                          workers=8, \n",
    "                                          use_multiprocessing=True,\n",
    "                                          )\n",
    "\n",
    "            for pidx, prediction in enumerate(predictions):\n",
    "\n",
    "                row_index = batches_processed * batch_size + pidx\n",
    "\n",
    "                lat = df.loc[row_index, 'doc_id'].split(':')[1]\n",
    "                lon = df.loc[row_index, 'doc_id'].split(':')[0]\n",
    "\n",
    "                table_row = {\n",
    "                        \"LATITUDE\": lat,\n",
    "                        \"LONGITUDE\": lon,\n",
    "                        \"ML_MODEL\": MODEL_NAME,\n",
    "                        \"ML_CONFIDENCE\": round(float(prediction[0]), 5),\n",
    "                        \"CLASSIFICATION_TYPE\": \"non-res\" if prediction < 0.5 else \"res\"\n",
    "                        }\n",
    "                \n",
    "                total_inferenced_count += 1\n",
    "                try:\n",
    "                    update_DB2_item(table_row, cursor)\n",
    "                    \n",
    "                except Exception as e:\n",
    "        \n",
    "                    print('Connection error, reconnect')\n",
    "\n",
    "                    DB2_connection = connect_to_db()\n",
    "                    cursor = DB2_connection.cursor()\n",
    "\n",
    "            batches_processed += 1\n",
    "            print('images infered & uploaded', len(images))\n",
    "            images, numeric = [], []\n",
    "            \n",
    "            inferenced_state['inferenced_count'] = batches_processed * batch_size\n",
    "            inferenced_state['current_file'] = fname[0]\n",
    "            inferenced_state['total_inferenced_count'] = total_inferenced_count\n",
    "            \n",
    "            t1 = time.time()\n",
    "            log_state_to_bucket(inferenced_state)\n",
    "#             print(time.time() - t1)\n",
    "               "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
