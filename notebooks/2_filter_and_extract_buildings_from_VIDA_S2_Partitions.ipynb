{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3_filter_and_extract_buildings_from_VIDA_S2_Partitions\n",
    "### An auxuliary notebook filtering buildings belonging into states withing countries (such as the State of Maharashtra in India)\n",
    "### Additionally it computes basic statistics about buildings based on their footprints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial configuration\n",
    "#### To start working with this particular notebook, you need to provide necessary credential and settings\n",
    "#### Below is an template of configuration, which is necessary prepare aside of this notebook and copy & paste all content in triple quotes to the next cell's input field\n",
    "#### Please make sure OSM provides boundary polygon for the specified country name!\n",
    "    \"\"\"\n",
    "    {\n",
    "    \"COS_ENDPOINT_URL\": \"s3.private.eu-de.cloud-object-storage.appdomain.cloud\",\n",
    "    \"COS_AUTH_ENDPOINT_URL\": \"https://iam.cloud.ibm.com/oidc/token\",\n",
    "    \"COS_APIKEY\": \"xxx\",\n",
    "    \"COUNTRY_NAME\": \"Maharastra\",\n",
    "    \"GRID_STORAGE_BUCKET\": \"xxx\",\n",
    "    \"VIDA_PARQUET_BUCKET\": \"parquets\",\n",
    "    \"UTILS_BUCKET\"; \"utils bucket name\",\n",
    "    \"VIDA_S2_PARTITIONS\"; \"VIDA_S2_PARTITIONS bucket\",\n",
    "    \"VIDA_COUNTRIES_BUILDINGS\"; \"vida-countries-buildings \",\n",
    "    }\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read notebook configuration\n",
    "import getpass\n",
    "import json\n",
    "\n",
    "config_str = getpass.getpass('Enter your prepared config: ')\n",
    "config = json.loads(config_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pyarrow.parquet as pq\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shapely\n",
    "import time\n",
    "import threading\n",
    "import datetime \n",
    "import jaydebeapi as jdbc\n",
    "import jpype\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "import io\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from pyproj import Geod\n",
    "\n",
    "geod = Geod(ellps=\"WGS84\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying to localStorage :  filtering_grid_generator.py\n",
      "Copying to localStorage :  india_state.geojson\n",
      "Copying to localStorage :  regions_S2_ids.pkl\n",
      "External utils succesfully imported\n"
     ]
    }
   ],
   "source": [
    "# init S3 client in order to work with last tiff file version\n",
    "cos_client = ibm_boto3.client(service_name='s3',\n",
    "                                  ibm_api_key_id=config[\"COS_APIKEY\"],\n",
    "                                  ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n",
    "                                  config=Config(signature_version='oauth'),\n",
    "                                  endpoint_url=config[\"COS_ENDPOINT_URL\"])\n",
    "\n",
    "\n",
    "# import external utils library\n",
    "response = cos_client.list_objects_v2(Bucket=config[\"UTILS_BUCKET\"])\n",
    "\n",
    "utils_to_download = ['regions_S2_ids.pkl', 'filtering_grid_generator.py', 'india_state.geojson']\n",
    "\n",
    "try:\n",
    "    for obj in response['Contents']:\n",
    "        name = obj['Key']\n",
    "        if name in utils_to_download:\n",
    "            streaming_body_1 = cos_client.get_object(Bucket=config[\"UTILS_BUCKET\"], Key=name)['Body']\n",
    "            print(\"Copying to localStorage :  \" + name)\n",
    "            with io.FileIO(name, 'w') as file:\n",
    "                for i in io.BytesIO(streaming_body_1.read()):\n",
    "                    file.write(i)\n",
    "                    \n",
    "    from filtering_grid_generator import GridGenerator\n",
    "    \n",
    "    print('External utils succesfully imported')\n",
    "except Exception as e:\n",
    "    print('Error occured: ', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download all the S2 partitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2_partitions_bucket_response = cos_client.list_objects_v2(Bucket=config[\"VIDA_S2_PARTITIONS\"]).get('Contents', [])\n",
    "S2_partitions_bucket_objects = [i['Key'] for i in S2_partitions_bucket_response]\n",
    "\n",
    "\n",
    "parquets_folder = 'S2_partitions'\n",
    "if not os.path.exists(parquets_folder):\n",
    "    os.makedirs(parquets_folder)\n",
    "    \n",
    "filtered_parquets_folder = 'states_filteded_parquets'\n",
    "if not os.path.exists(filtered_parquets_folder):\n",
    "    os.makedirs(filtered_parquets_folder)\n",
    "    \n",
    "for f in tqdm(S2_partitions_bucket_objects, total=len(S2_partitions_bucket_objects)):\n",
    "    \n",
    "    try:\n",
    "        cos_client.download_file(Bucket=config[\"VIDA_S2_PARTITIONS\"],Key=f,Filename=f'{parquets_folder}/{f}')\n",
    "    except Exception as e:\n",
    "        print(f'While downloading exception occurred: {e}')\n",
    "        try:\n",
    "            \n",
    "            time.sleep(5)\n",
    "            cos_client.download_file(Bucket=config[\"VIDA_S2_PARTITIONS\"],Key=f,Filename=f'{parquets_folder}/{f}')\n",
    "        except Exception as e:\n",
    "            print(f'While second downloading attempt exception occurred: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Madhya Pradesh': [<POLYGON ((78.365 26.869, 78.367 26.863, 78.37 26.858, 78.375 26.847, 78.381...>],\n",
       " 'South-India': [<POLYGON ((80.076 13.527, 80.076 13.526, 80.079 13.529, 80.087 13.527, 80.08...>,\n",
       "  <POLYGON ((74.996 12.788, 75 12.783, 75.004 12.786, 75.005 12.785, 75.004 12...>],\n",
       " 'East-India': [<POLYGON ((87.6 25.315, 87.607 25.311, 87.614 25.316, 87.623 25.311, 87.626 ...>,\n",
       "  <POLYGON ((95.214 26.937, 95.217 26.934, 95.226 26.934, 95.229 26.931, 95.23...>,\n",
       "  <POLYGON ((92.801 24.419, 92.804 24.419, 92.807 24.42, 92.809 24.419, 92.809...>,\n",
       "  <POLYGON ((95.952 27.942, 95.952 27.939, 95.952 27.937, 95.958 27.937, 95.95...>]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "india_states_df = gpd.read_file('india_state.geojson')\n",
    "\n",
    "\n",
    "regions_polygons = {\n",
    "    'Madhya Pradesh': [\n",
    "        india_states_df[india_states_df.NAME_1 == 'Madhya Pradesh'].geometry.iloc[0]\n",
    "        ],\n",
    "    'South-India': [\n",
    "        india_states_df[india_states_df.NAME_1 == 'Tamil Nadu'].geometry.iloc[0].geoms[-1],\n",
    "        india_states_df[india_states_df.NAME_1 == 'Kerala'].geometry.iloc[0].geoms[-1]\n",
    "    ],\n",
    "    'East-India': [\n",
    "        india_states_df[india_states_df.NAME_1 == 'Jharkhand'].geometry.iloc[0],\n",
    "        india_states_df[india_states_df.NAME_1 == 'Nagaland'].geometry.iloc[0],\n",
    "        india_states_df[india_states_df.NAME_1 == 'Mizoram'].geometry.iloc[0],\n",
    "        india_states_df[india_states_df.NAME_1 == 'Assam'].geometry.iloc[0].geoms[-1],\n",
    "    ]\n",
    "}\n",
    "\n",
    "regions_polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_S2_ids = pickle.load(open('regions_S2_ids.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = GridGenerator()\n",
    "\n",
    "for region_name, region_polygons in regions_polygons.items():\n",
    "    \n",
    "    print('\\033[96mProcessing region', region_name)\n",
    "    \n",
    "    region_S2_ids = regions_S2_ids[region_name]\n",
    "    \n",
    "    # buildings_inside_region = []\n",
    "    \n",
    "    for pidx, region_polygon in enumerate(region_polygons):\n",
    "        \n",
    "        print('\\033[93m    processing pidx', pidx)\n",
    "        \n",
    "        inside_polygon_grid, outside_polygon_grid = grids.generate_grids(region_polygon, n=150)\n",
    "        \n",
    "        # buildings_inside_polygon = []\n",
    "        \n",
    "        buildings_inside_polygon_found = 0\n",
    "        \n",
    "        for s2_idx, S2_id in enumerate(region_S2_ids):\n",
    "            \n",
    "            try:\n",
    "                file_name = f'India_{S2_id}.parquet'\n",
    "                \n",
    "                print(f'\\033[93m        processing file_name {file_name} idx {s2_idx} of {len(region_S2_ids)}')\n",
    "                \n",
    "                df = gpd.read_parquet(os.path.join(parquets_folder, file_name))\n",
    "                df['longitude'] = df['geometry'].apply(lambda g: g.centroid.xy[0][0])\n",
    "                df['latitude'] = df['geometry'].apply(lambda g: g.centroid.xy[1][0])\n",
    "                \n",
    "                df['id'] = df['longitude'].astype(str) + ':' + df['latitude'].astype(str)\n",
    "                print(f'\\033[93m        Amount of buildings in S2 partition {len(df)}')\n",
    "                \n",
    "                buildings_in_tiles = []\n",
    "                buildings_out_tiles = []\n",
    "                \n",
    "                buildings_found = 0\n",
    "                buildings_out_tiles_found = 0\n",
    "                \n",
    "                # first step fast filteing all buildings that located in polygon grid tiles\n",
    "                for in_poly_tile in tqdm(inside_polygon_grid, total=len(inside_polygon_grid), desc='Filtering buildings inside polygon'):\n",
    "                    \n",
    "                    min_lon, min_lat, max_lon, max_lat = in_poly_tile.bounds\n",
    "                    # Filter buildings within current tile\n",
    "                    buildings_in_tile = df[\n",
    "                            (df.longitude >= min_lon) &\n",
    "                            (df.longitude <= max_lon) &\n",
    "                            (df.latitude >= min_lat) &\n",
    "                            (df.latitude <= max_lat)\n",
    "                    ]\n",
    "                    \n",
    "                    if len(buildings_in_tile) > 0:\n",
    "                        \n",
    "                        buildings_in_tiles.append(buildings_in_tile)\n",
    "                        buildings_found += len(buildings_in_tile)\n",
    "                        \n",
    "                # first step fast filteing all buildings that located out polygon grid tiles       \n",
    "                for off_poly_tile in tqdm(outside_polygon_grid, total=len(outside_polygon_grid), desc='Filtering buildings outside polygon'):\n",
    "                    \n",
    "                    min_lon, min_lat, max_lon, max_lat = off_poly_tile.bounds\n",
    "                    # Filter buildings within current tile\n",
    "                    buildings_out_tile = df[\n",
    "                            (df.longitude >= min_lon) &\n",
    "                            (df.longitude <= max_lon) &\n",
    "                            (df.latitude >= min_lat) &\n",
    "                            (df.latitude <= max_lat)\n",
    "                    ]\n",
    "                    \n",
    "                    if len(buildings_out_tile) > 0:\n",
    "\n",
    "                        buildings_out_tiles.append(buildings_out_tile)\n",
    "                        buildings_out_tiles_found += len(buildings_out_tile)\n",
    "                \n",
    "                # Concatenate dataframes containing buildins within and outside tiles\n",
    "                \n",
    "                try:\n",
    "                    buildings_in_tiles_df = pd.concat(buildings_in_tiles)\n",
    "                except:\n",
    "                    buildings_in_tiles_df = pd.DataFrame()\n",
    "                \n",
    "                try:\n",
    "                    buildings_out_tiles_df = pd.concat(buildings_out_tiles)\n",
    "                except:\n",
    "                    buildings_out_tiles_df = pd.DataFrame()\n",
    "                \n",
    "                # check in out buildings of polygon\n",
    "                if (len(buildings_in_tiles_df) != 0) and (len(buildings_out_tiles_df) != 0):\n",
    "                \n",
    "                    try:\n",
    "                        print(f'\\033[93m        Buildings found in polygon tiles {len(buildings_in_tiles_df)} out tiles {len(buildings_out_tiles_df)} remaining {len(df) - len(buildings_in_tiles_df) - len(buildings_out_tiles_df)}')\n",
    "                        \n",
    "                        if len(buildings_in_tiles_df) > 0:\n",
    "                            remaining_buildings_df = df[~df['id'].isin(buildings_in_tiles_df['id'])]\n",
    "                            \n",
    "                            if len(buildings_out_tiles_df) > 0:\n",
    "                                remaining_buildings_df = remaining_buildings_df[~remaining_buildings_df['id'].isin(buildings_out_tiles_df['id'])]\n",
    "                                \n",
    "                        if (len(buildings_in_tiles_df) == 0) and (len(buildings_out_tiles_df) > 0):\n",
    "                            remaining_buildings_df = df[~df['id'].isin(buildings_out_tiles_df['id'])]\n",
    "                            \n",
    "                        \n",
    "                        print(f'\\033[93m        Remaining amount check {len(remaining_buildings_df)}')\n",
    "                        \n",
    "                        result_df = pd.DataFrame()\n",
    "                        \n",
    "                        if len(remaining_buildings_df) > 0:\n",
    "                            remaining_buildings_df['buildings_in_polygon'] = [region_polygon.contains(shapely.Point(row.longitude, row.latitude)) for row in tqdm(remaining_buildings_df.itertuples(), total=len(remaining_buildings_df), desc='Filtering buildings')]\n",
    "\n",
    "                            remaining_buildings_df = remaining_buildings_df[remaining_buildings_df.buildings_in_polygon == True]\n",
    "                            \n",
    "                            if len(buildings_in_tiles_df) > 0:\n",
    "                                if len(remaining_buildings_df) > 0:\n",
    "                                    result_df = pd.concat([buildings_in_tiles_df, remaining_buildings_df])\n",
    "                                else:\n",
    "                                    result_df = buildings_in_tiles_df\n",
    "                            else:\n",
    "                                if len(remaining_buildings_df) > 0:\n",
    "                                    result_df = remaining_buildings_df\n",
    "                                else:\n",
    "                                    result_df = pd.DataFrame()\n",
    "                        else:\n",
    "                            if len(buildings_in_tiles_df) > 0:\n",
    "                                result_df = buildings_in_tiles_df\n",
    "                            else:\n",
    "                                result_df = pd.DataFrame()\n",
    "                                \n",
    "                        if len(result_df) > 0:\n",
    "                            print(f'\\033[93m        Saved amount {len(result_df)}')\n",
    "                            result_df.to_parquet(f\"{filtered_parquets_folder}/{region_name.replace(' ', '_')}_pidx_{pidx}_S2_id_{S2_id}_buildings.parquet\")\n",
    "                        # buildings_inside_polygon.append(result_df)\n",
    "                        \n",
    "                        print()\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Madhya Pradesh', 'South-India', 'East-India'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regions_polygons.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inner_faces(interiors):\n",
    "    total = 0\n",
    "    for interior in interiors:\n",
    "        total += len(interior.coords) - 1\n",
    "    return total\n",
    "\n",
    "def get_inner_perimeter(interiors):\n",
    "    geod = Geod(ellps=\"WGS84\")\n",
    "    total = 0\n",
    "    for interior in interiors:\n",
    "        total += abs(geod.geometry_area_perimeter(interior)[1])\n",
    "    return total\n",
    "\n",
    "\n",
    "def calculations(buildings_in_polygon):\n",
    "    \n",
    "    # filter out google buildings vith cobfidence < 0.7\n",
    "    google_buildings = buildings_in_polygon[(buildings_in_polygon.bf_source == 'google') & (buildings_in_polygon.confidence > 0.7)]\n",
    "    microsoft_buildings = buildings_in_polygon[buildings_in_polygon.bf_source == 'microsoft']\n",
    "    \n",
    "    buildings_in_polygon = pd.concat([google_buildings, microsoft_buildings])\n",
    "    \n",
    "    del google_buildings\n",
    "    del microsoft_buildings\n",
    "    \n",
    "    buildings_in_polygon['area_in_meters'] = buildings_in_polygon[\"geometry\"].apply(lambda g: abs(geod.geometry_area_perimeter(g)[0]))\n",
    "\n",
    "    # Compute Perimeter\n",
    "    buildings_in_polygon.insert(0,\"perimeter_in_meters\",0)\n",
    "    buildings_in_polygon['perimeter_in_meters'] = buildings_in_polygon[\"geometry\"].apply(lambda g: (abs(geod.geometry_area_perimeter(g)[1]) + get_inner_perimeter(g.interiors)) if (g.geom_type == \"Polygon\") else (abs(geod.geometry_area_perimeter(g)[1])))\n",
    "\n",
    "    # Compute number of faces\n",
    "    buildings_in_polygon.insert(1,\"building_faces\",0)\n",
    "    buildings_in_polygon['building_faces'] = buildings_in_polygon[\"geometry\"].apply(lambda g: (len(g.exterior.coords) - 1 + get_inner_faces(g.interiors)) if (g.geom_type == \"Polygon\") else 0)\n",
    "\n",
    "    \n",
    "    return buildings_in_polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filtered_partitions = os.listdir(filtered_parquets_folder)\n",
    "columns = ['bf_source', 'confidence', 'geometry', 'longitude', 'latitude', 'id']\n",
    "\n",
    "for region_name in regions_polygons.keys():\n",
    "\n",
    "    region_name = region_name.replace(' ', '_')\n",
    "    try:\n",
    "        filtered_partitions = [i for i in filtered_partitions if region_name in i]\n",
    "\n",
    "        main_df = gpd.read_parquet(os.path.join(filtered_parquets_folder, filtered_partitions[0]), columns=columns)\n",
    "\n",
    "        for i in filtered_partitions[1:]:\n",
    "            \n",
    "            parquet_path = os.path.join(filtered_parquets_folder, i)\n",
    "            current_df = gpd.read_parquet(parquet_path, columns=columns)\n",
    "            main_df = pd.concat([main_df, current_df])\n",
    "\n",
    "        main_df = main_df.drop_duplicates(subset=['id'])\n",
    "        \n",
    "        filename = f'{region_name}_buildings.parquet'\n",
    "        \n",
    "        main_df = calculations(main_df)\n",
    "        main_df.to_parquet(filename)\n",
    "        \n",
    "        # upload to bucket\n",
    "        try:\n",
    "            res=cos_client.upload_file(Filename=filename, Bucket=config[\"VIDA_COUNTRIES_BUILDINGS\"],Key=filename)\n",
    "        except Exception as e:\n",
    "            print(Exception, e)\n",
    "        else:\n",
    "            print(f'{filename} succesfully uploaded')\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f'Region parquets concatenation error occurred: {e}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
