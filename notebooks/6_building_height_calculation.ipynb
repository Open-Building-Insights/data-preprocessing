{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7_building_height_calculation\n",
    "### The source for building height calculation arrives from DLR and is uploaded to a dedicated bucket in advance manually. Given that this source is not public at the current moment it is handled manually, the code consequently reads the TIFF file uploaded to the bucket as the input\n",
    "### This notebook estimates the heights of buildings based on the provided raster layer (10x10m per pixel resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial configuration\n",
    "#### To start working with this particular notebook, you need to provide necessary credential and settings\n",
    "#### Below is an template of configuration, which is necessary prepare aside of this notebook and copy & paste all content in triple quotes to the next cell's input field\n",
    "    \"\"\"\n",
    "    {\n",
    "    \"COS_ENDPOINT_URL\": \"s3.private.eu-de.cloud-object-storage.appdomain.cloud\",\n",
    "    \"COS_AUTH_ENDPOINT_URL\": \"https://iam.cloud.ibm.com/oidc/token\",\n",
    "    \"COS_APIKEY\": \"xxx\",\n",
    "    \"UTILS_BUCKET\": \"notebook-utils-bucket\",\n",
    "    \"HEIGHTS_TIFF_FILENAME\": \"WSF3Dv3_Kenya.tif\",\n",
    "    \"DB2_CONNECTION_STRING\": \"jdbc:db2://65beb513-5d3d-4101-9001-f42e9dc954b3.brt9d04f0cmqeb8u7740.databases.appdomain.cloud:30371/BLUDB:sslConnection=true;useJDBC4ColumnNameAndLabelSemantics=false;db2.jcc.charsetDecoderEncoder=3;\",\n",
    "    \"DB2_USERNAME\": \"xxx\",\n",
    "    \"DB2_PASSWORD\": \"xxx\",\n",
    "    \"COUNTRY_TABLE\": \"FEATURES_DB_VIDA_EXTENDED\",\n",
    "    \"BUCKET_TIFF\": \"buildings-height-tiffs\",\n",
    "    \"AREA_THRESHOLD\": 20\n",
    "    }\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read notebook configuration\n",
    "import getpass\n",
    "import json\n",
    "\n",
    "config_str = getpass.getpass('Enter your prepared config: ')\n",
    "config = json.loads(config_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying to localStorage :  db2jcc4.jar\n",
      "Copying to localStorage :  utils.py\n",
      "External utils succesfully imported\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import io\n",
    "from PIL import Image\n",
    "import ibm_boto3\n",
    "import jaydebeapi as jdbc\n",
    "import jpype\n",
    "from botocore.client import Config\n",
    "import numpy as np\n",
    "import configparser\n",
    "import os\n",
    "import sys\n",
    "from ibm_cloud_sdk_core import ApiException\n",
    "from ibmcloudant.cloudant_v1 import CloudantV1, Document, BulkDocs\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import random\n",
    "import time\n",
    "import base64\n",
    "import shutil\n",
    "import threading\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "import gc\n",
    "from matplotlib.path import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# init S3 client in order to work with last tiff file version\n",
    "cos_client = ibm_boto3.client(service_name='s3',\n",
    "                              ibm_api_key_id=config[\"COS_APIKEY\"],\n",
    "                              ibm_auth_endpoint=config[\"COS_AUTH_ENDPOINT_URL\"],\n",
    "                              config=Config(signature_version='oauth'),\n",
    "                              endpoint_url=config[\"COS_ENDPOINT_URL\"])\n",
    "\n",
    "\n",
    "# import external utils library\n",
    "response = cos_client.list_objects_v2(Bucket=config[\"UTILS_BUCKET\"])\n",
    "\n",
    "try:\n",
    "    for obj in response['Contents']:\n",
    "        name = obj['Key']\n",
    "        streaming_body_1 = cos_client.get_object(Bucket=config[\"UTILS_BUCKET\"], Key=name)['Body']\n",
    "        print(\"Copying to localStorage :  \" + name)\n",
    "        with io.FileIO(name, 'w') as file:\n",
    "            for i in io.BytesIO(streaming_body_1.read()):\n",
    "                file.write(i)\n",
    "    \n",
    "    from utils import *\n",
    "    print('External utils succesfully imported')\n",
    "except Exception as e:\n",
    "    print('Error occured: ', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/wsuser/ipykernel_136/1572909626.py:14: DeprecationWarning: jpype._core.isThreadAttachedToJVM is deprecated, use java.lang.Thread.isAttached instead\n",
      "  if jpype.isJVMStarted() and not jpype.isThreadAttachedToJVM():\n"
     ]
    }
   ],
   "source": [
    "# connect to the IBM DB2 function\n",
    "def connect_to_db():\n",
    "\n",
    "    jar = 'db2jcc4.jar'\n",
    "#     os.environ['JAVA_HOME'] = '/usr/libexec/java_home'\n",
    "    os.environ['CLASSPATH'] = jar\n",
    "\n",
    "    args='-Djava.class.path=%s' % jar\n",
    "    jvm_path = jpype.getDefaultJVMPath()\n",
    "    try:\n",
    "        jpype.startJVM(jvm_path, args)\n",
    "    except Exception as e:\n",
    "        print('startJVM exception: ', e)\n",
    "        \n",
    "    if jpype.isJVMStarted() and not jpype.isThreadAttachedToJVM():\n",
    "        jpype.attachThreadToJVM()\n",
    "        jpype.java.lang.Thread.currentThread().setContextClassLoader(jpype.java.lang.ClassLoader.getSystemClassLoader())\n",
    "        \n",
    "    # create JDBC connection\n",
    "    conn = jdbc.connect(\n",
    "                'com.ibm.db2.jcc.DB2Driver',\n",
    "                config['DB2_CONNECTION_STRING'],\n",
    "                [config[\"DB2_USERNAME\"], config[\"DB2_PASSWORD\"]],\n",
    "                'db2jcc4.jar')\n",
    "    \n",
    "\n",
    "    return conn\n",
    "\n",
    "DB2_connection = connect_to_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign config necessary variables\n",
    "BUCKET_TIFF = config[\"BUCKET_TIFF\"]\n",
    "table_name = config[\"COUNTRY_TABLE\"]\n",
    "heights_tiff_name = config[\"HEIGHTS_TIFF_FILENAME\"]\n",
    "area_threshold = config[\"AREA_THRESHOLD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recreate /tiff directories\n",
      "Copying to localStorage: tiff/WSF3Dv3_Kenya.tif\n",
      "Successfully downloaded\n"
     ]
    }
   ],
   "source": [
    "# paths for local storing of tiff files\n",
    "path_to_tif_folder = 'tiff/' \n",
    "processed_tiff = 'processd_tiffs/'\n",
    "\n",
    "# clear files in directories if exist\n",
    "try:\n",
    "    shutil.rmtree(path_to_tif_folder, ignore_errors=True)\n",
    "    shutil.rmtree(processed_tiff, ignore_errors=True)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "print(\"Recreate /tiff directories\")\n",
    "os.makedirs(os.path.dirname(path_to_tif_folder), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(processed_tiff), exist_ok=True)\n",
    "\n",
    "# download heights tiff file\n",
    "streaming_body = cos_client.get_object(Bucket=BUCKET_TIFF, Key=heights_tiff_name)['Body']      \n",
    "with io.FileIO(path_to_tif_folder + heights_tiff_name, 'w') as file:\n",
    "    print(\"Copying to localStorage: \" + path_to_tif_folder + heights_tiff_name)\n",
    "    for i in io.BytesIO(streaming_body.read()):\n",
    "        file.write(i)\n",
    "\n",
    "print('Successfully downloaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'driver': 'GTiff', 'dtype': 'float64', 'nodata': None, 'width': 111543, 'height': 133808, 'count': 1, 'crs': CRS.from_epsg(4326), 'transform': Affine(8.983152841195211e-05, 0.0, 31.98999541430869,\n",
      "       0.0, -8.983152841195211e-05, 6.010088576873247), 'blockysize': 1, 'tiled': False, 'compress': 'lzw', 'interleave': 'band'}\n",
      "tile_width: 11154, tile_height: 13380\n",
      "TIFf image wiil be divided to 11 rows and 11 cols\n",
      "121\n"
     ]
    }
   ],
   "source": [
    "#open heights tiff \n",
    "dat = rasterio.open(os.path.join(path_to_tif_folder, heights_tiff_name))\n",
    "profile = dat.profile.copy()\n",
    "profile.update(compress='lzw')\n",
    "print(profile)\n",
    "\n",
    "#divide tiff to tiles\n",
    "tiff_width = profile['width']\n",
    "tiff_height = profile['height']\n",
    "\n",
    "tile_width = int(tiff_width / 10)\n",
    "tile_height = int(tiff_height / 10)\n",
    "\n",
    "print(f'tile_width: {tile_width}, tile_height: {tile_height}')\n",
    "# define overlap between tiles\n",
    "overlap = 500\n",
    "\n",
    "columns_amount = int(tiff_width / tile_width) if tiff_width % tile_width == 0 else int(tiff_width / tile_width) + 1\n",
    "rows_amount = int(tiff_height / tile_height) if tiff_height % tile_height == 0 else int(tiff_height / tile_height) + 1\n",
    "print(f'TIFf image wiil be divided to {rows_amount} rows and {columns_amount} cols')\n",
    "\n",
    "images_coords = []\n",
    "\n",
    "for col_idx in range(1, columns_amount + 1):\n",
    "    \n",
    "    row_start = max(tile_width * (col_idx - 1) - overlap, 0)\n",
    "    \n",
    "    if col_idx != columns_amount:\n",
    "        \n",
    "        row_limits = [row_start, tile_width * col_idx]\n",
    "    elif col_idx == columns_amount:\n",
    "        row_limits = [row_start, tiff_width]\n",
    "    \n",
    "    for row_idx in range(1, rows_amount + 1):\n",
    "        \n",
    "        col_start = max(tile_height * (row_idx - 1) - overlap, 0)\n",
    "        \n",
    "        if row_idx != columns_amount:\n",
    "            col_limits = [col_start, tile_height * row_idx]\n",
    "        elif row_idx == columns_amount:\n",
    "            col_limits = [col_start, tiff_height]\n",
    "            \n",
    "        coords = [col_limits, row_limits]\n",
    "        images_coords.append(coords)\n",
    "print(len(images_coords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare GeoDataframe of buildings from bounding box\n",
    "def fetch_builings_in_bbox(lon_min, lon_max, lat_min, lat_max):\n",
    "\n",
    "    sql = f\"\"\"\n",
    "        SELECT DISTINCT ID, POLYGON_COORDINATES, AREA_IN_METERS FROM USER1.{table_name} \n",
    "        WHERE \n",
    "            (LATITUDE >= {lat_min}) AND \n",
    "            (LATITUDE <= {lat_max}) AND \n",
    "            (LONGITUDE >= {lon_min}) AND \n",
    "            (LONGITUDE <= {lon_max}) AND\n",
    "            AREA_IN_METERS > {area_threshold}\n",
    "            \"\"\"\n",
    "    cursor = DB2_connection.cursor()\n",
    "    cursor.execute(sql)\n",
    "    data = cursor.fetchall()\n",
    "\n",
    "    gpd.options.display_precision = 7 \n",
    "\n",
    "    df = pd.DataFrame(data=data, columns=['doc_id', 'polygon_coordinates'])\n",
    "\n",
    "    convert_dict = {\n",
    "                    'doc_id': str,\n",
    "                    'polygon_coordinates': str,\n",
    "                    'area_in_meters': float\n",
    "                    }\n",
    "\n",
    "    df = df.astype(convert_dict)\n",
    "    df['geometry'] = gpd.GeoSeries.from_wkt(df['polygon_coordinates'])\n",
    "    df = df.drop(columns=['polygon_coordinates'])\n",
    "\n",
    "    df = gpd.GeoDataFrame(\n",
    "        df, geometry=df.geometry, crs=\"EPSG:4326\"\n",
    "    )\n",
    "\n",
    "    df = df.where(~df['geometry'].isna()).dropna()\n",
    "\n",
    "    df[\"corresponding_tiff\"] = ['NA' for _ in range(len(df))]\n",
    "    df \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upd_default_height_in_bbox(lon_min, lon_max, lat_min, lat_max):\n",
    "\n",
    "    try:\n",
    "        sql = f\"\"\"\n",
    "        UPDATE \"USER1\".\"{table_name}\"\n",
    "            SET\n",
    "                \"HEIGHT\" = 3,   \n",
    "                \"HEIGHT_MEDIAN\" = 3, \n",
    "                \"HEIGHT_MEAN\" = 3, \n",
    "                \"HEIGHT_MAX\" = 3,\n",
    "                \"FLOORS\" = 1,\n",
    "                \"GFA_IN_METERS\" = \"AREA_IN_METERS\"\n",
    "            WHERE \n",
    "                (LATITUDE >= {lat_min}) AND \n",
    "                (LATITUDE <= {lat_max}) AND \n",
    "                (LONGITUDE >= {lon_min}) AND \n",
    "                (LONGITUDE <= {lon_max}) AND\n",
    "                AREA_IN_METERS <= {area_threshold}\n",
    "        \"\"\"\n",
    "        cursor = DB2_connection.cursor()\n",
    "        cursor.execute(sql)\n",
    "    except Exception as e:\n",
    "        print(e, sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upd_height_db2(lat, lon, height, height_max, height_mean, height_median, floors, GFA_in_meters, cursor):\n",
    "\n",
    "    try:\n",
    "        sql = f\"\"\"\n",
    "        UPDATE \"USER1\".\"{table_name}\"\n",
    "            SET\n",
    "                \"HEIGHT\" = {height},   \n",
    "                \"HEIGHT_MEDIAN\" = {height_median}, \n",
    "                \"HEIGHT_MEAN\" = {height_mean}, \n",
    "                \"HEIGHT_MAX\" = {height_max},\n",
    "                \"FLOORS\" = {floors},\n",
    "                \"GFA_IN_METERS\" = {GFA_in_meters}\n",
    "            WHERE \n",
    "                (\"LATITUDE\" = {lat}) AND \n",
    "                (\"LONGITUDE\" = {lon})\n",
    "        \"\"\"\n",
    "        cursor.execute(sql)\n",
    "    except Exception as e:\n",
    "        print(e, sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through all tiles and calculate heights stats for all available buildings in tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cursor = DB2_connection.cursor()\n",
    "dfs = []\n",
    "t1 = time.time()\n",
    "\n",
    "# loop through tiles coords\n",
    "for idx, coords in enumerate(images_coords):\n",
    "    init_time = time.time()\n",
    "    with rasterio.open(os.path.join(path_to_tif_folder, heights_tiff_name)) as src:\n",
    "        \n",
    "        # read tiff metadata by coords in order to prepare filtered dataframe\n",
    "        print(coords)\n",
    "        \n",
    "        col_off = coords[1][0]\n",
    "        row_off = coords[0][0]\n",
    "        \n",
    "        width = coords[1][1] - coords[1][0]\n",
    "        height = coords[0][1] - coords[0][0]\n",
    "        \n",
    "        tiff_data = src.read(1, window=Window(col_off, row_off, width, height))\n",
    "        \n",
    "        lon_upper_left, lat_upper_left = src.xy(coords[0][0], coords[1][0])\n",
    "        lon_down_right, lat_down_right = src.xy(coords[0][1], coords[1][1])\n",
    "\n",
    "        lons_sorted = sorted([lon_upper_left, lon_down_right])\n",
    "        lats_sorted = sorted([lat_upper_left, lat_down_right])\n",
    "        \n",
    "        lon_min = lons_sorted[0]\n",
    "        lon_max = lons_sorted[1]\n",
    "\n",
    "        lat_min = lats_sorted[0]\n",
    "        lat_max = lats_sorted[1]\n",
    "        \n",
    "        areas_covered_by_tifs = create_bounds_dict(path_to_tifs=path_to_tif_folder)\n",
    "\n",
    "        # set up default height (3m - 1 floor) for buildings under the threshold (20 square meters)\n",
    "        upd_default_height_in_bbox(lon_min, lon_max, lat_min, lat_max)\n",
    "\n",
    "        # fetch all buildings larger than the threshold (20 square meters) to estimate its height\n",
    "        df = fetch_builings_in_bbox(lon_min, lon_max, lat_min, lat_max)\n",
    "\n",
    "        # coordinates of current tile\n",
    "        col_off = max(coords[1][0] - 1, 0)\n",
    "        row_off = max(coords[0][0] - 1, 0)\n",
    "        width = min(coords[1][1] - coords[1][0] + 1, tiff_width)\n",
    "        height = min(coords[0][1] - coords[0][0] + 1, tiff_height)\n",
    "        \n",
    "        # read tile grayscale layer\n",
    "        tiff_data = src.read(1, window=Window(col_off, row_off, width, height))\n",
    "        print(f\"Images revealed: {len(df)}\")\n",
    "        \n",
    "        # loop through building centroids inside tile\n",
    "        for index, row, in tqdm(df.iterrows(), total=len(df), desc='Height calculation'):\n",
    "            try:\n",
    "\n",
    "                # lat lon to pixel transformations\n",
    "                pixel_coordinates = get_pixel_coordinates(row.geometry, areas_covered_by_tifs, src)\n",
    "                polygon_coordinates = [[pixel_coords[0] - row_off, pixel_coords[1] - col_off] for pixel_coords in pixel_coordinates]\n",
    "                \n",
    "                margin = 0\n",
    "                \n",
    "                rowcolminmax = get_min_max_values_of_row_col(pixel_coordinates=polygon_coordinates)\n",
    "                \n",
    "                img_width = rowcolminmax['rowminmax'][1] - rowcolminmax['rowminmax'][0] \n",
    "                img_height = rowcolminmax['colminmax'][1] - rowcolminmax['colminmax'][0]\n",
    "                \n",
    "                row_start = rowcolminmax['rowminmax'][0]\n",
    "                row_end = rowcolminmax['rowminmax'][1]\n",
    "                col_start = rowcolminmax['colminmax'][0]\n",
    "                col_end = rowcolminmax['colminmax'][1]\n",
    "                img_array_pre = np.array(tiff_data[row_start : row_end, col_start : col_end])\n",
    "                \n",
    "\n",
    "                polygon_coordinates = offset_polygon_coords(polygon_coordinates)\n",
    "                rowcolminmax = get_min_max_values_of_row_col(pixel_coordinates=polygon_coordinates)\n",
    "\n",
    "                img_width = rowcolminmax['rowminmax'][1] - rowcolminmax['rowminmax'][0] \n",
    "                img_height = rowcolminmax['colminmax'][1] - rowcolminmax['colminmax'][0]\n",
    "                row_start = rowcolminmax['rowminmax'][0]\n",
    "                row_end = rowcolminmax['rowminmax'][1]\n",
    "                col_start = rowcolminmax['colminmax'][0]\n",
    "                col_end = rowcolminmax['colminmax'][1]\n",
    "\n",
    "                # cut building image from tile\n",
    "                img_array_pre = np.array(tiff_data[row_start : row_end, col_start : col_end])\n",
    "                \n",
    "                # check if cropped image correcspond with init image shape,\n",
    "                # if not place cropped image on appropriate image matriz size\n",
    "                if img_array_pre.shape != (img_width, img_height):\n",
    "                        print('building polygon out of tiff')\n",
    "                        np_nan_matrix = np.empty((img_width, img_height))\n",
    "                        np_nan_matrix.fill(np.nan)\n",
    "                        np_nan_matrix[:img_array_pre.shape[0], :img_array_pre.shape[1]] = img_array_pre\n",
    "                        img_array_pre = np_nan_matrix\n",
    "                \n",
    "                # extract building by polygon coords\n",
    "                absolule_polygon_coordinates = [[pixel_coords[0] - row_start, pixel_coords[1] - col_start] for pixel_coords in polygon_coordinates]\n",
    "                poly_path=Path(absolule_polygon_coordinates)\n",
    "                x, y = np.mgrid[:img_height, :img_width]\n",
    "                coors = np.hstack((x.reshape(-1, 1), y.reshape(-1,1)))\n",
    "                mask = poly_path.contains_points(coors).reshape(img_height, img_width).T\n",
    "                \n",
    "                # create zeros mask\n",
    "                img_masked=np.zeros((img_width, img_height),dtype=img_array_pre.dtype)\n",
    "\n",
    "                # put image on zeros mask\n",
    "                img_masked[mask]=img_array_pre[mask]\n",
    "\n",
    "                # extract image as list of non zero values\n",
    "                img_masked_list = list(filter(lambda num: num != 0, img_masked.flatten(order='C')))\n",
    "\n",
    "                #calculate statistics \n",
    "                \n",
    "                height_categorized = 3 if len(img_masked_list) == 0 else np.median(img_masked_list) // 3 * 3 + 3\n",
    "                floors = int(height_categorized / 3)\n",
    "                gfa = round(row.area_in_meters * floors, 5)\n",
    "                \n",
    "                df.at[index, 'height_mean_by_poly'] = 0 if len(img_masked_list) == 0 else np.mean(img_masked_list)\n",
    "                df.at[index, 'height_median_by_poly'] = 0 if len(img_masked_list) == 0 else np.median(img_masked_list)\n",
    "                df.at[index, 'height_max_by_poly'] = 0 if len(img_masked_list) == 0 else np.max(img_masked_list)\n",
    "                df.at[index, 'height_categorized'] = height_categorized\n",
    "\n",
    "                df.at[index, 'floors'] = floors\n",
    "                df.at[index, 'GFA_in_meters'] = gfa\n",
    "                \n",
    "            except Exception as e:\n",
    "                pass\n",
    "                \n",
    "        #Update changed height values        \n",
    "        for row in tqdm(df.itertuples(), total=len(df), desc='ingestion_data'):\n",
    "            lon, lat = row.doc_id.split(':')\n",
    "            try:\n",
    "                upd_height_db2(lat, lon, row.height_categorized, row.height_max_by_poly, row.height_mean_by_poly, row.height_median_by_poly, row.floors, row.GFA_in_meters, cursor)\n",
    "            except Exception as e:\n",
    "                print(f\"Error of database: {e}\")\n",
    "                DB2_connection = connect_to_db()\n",
    "                cursor = DB2_connection.cursor()\n",
    "                upd_height_db2(lat, lon, row.height_categorized, row.height_max_by_poly, row.height_mean_by_poly, row.height_median_by_poly, row.floors, row.GFA_in_meters, cursor)\n",
    "        print(f'Image tile processed, time took: {time.strftime(\"%H:%M:%S\", time.gmtime(int(time.time() - init_time)))}')\n",
    "#         if idx == 16:\n",
    "#             break\n",
    "        \n",
    "        \n",
    "        \n",
    "print(\"\")\n",
    "print(f'All tiles processed, time took: {time.strftime(\"%H:%M:%S\", time.gmtime(int(time.time() - t1)))}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
